{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple \"Wellesley News\" Search Results Scraper\n",
    "Shreya Parjan\n",
    "\n",
    "10/31/19\n",
    "\n",
    "This scraper is designed for use with the Wellesley College News website. It takes in a results page from a search bar search and outputs a CSV that lists the title and date for the articles on that page. Ultimately, the outputs from this scraper can be used to analyze trends in titles/frequency of discussion of certain issues. For this proof of concept, as a part of our group project to create a dataset to quantify conversations about student housing, we examine a page of results from the search term \"housing.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. [Step 1: Scraping Environment Set-up](#s0)\n",
    "2. [Step 2: Isolating Content](#s1)\n",
    "3. [Step 3: Extracting Dates with RegEx](#s2)\n",
    "4. [Step 4: Extracting Titles with String Indexing](#s3)\n",
    "5. [Step 5: Writing Data to CSV](#s4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Scraping Environment Set-up\n",
    "<a id=\"s0\"></a>\n",
    "\n",
    "We use Python's BeautifulSoup package and the requests library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through requests, we specify our URL and extract all of the content in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://thewellesleynews.com/page/1/?s=housing\"\n",
    "r = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Isolating Content\n",
    "<a id=\"s1\"></a>\n",
    "\n",
    "Clearly, the content above is too dense for us to parse in a meaningful way and even worse for converting into an actionable spreadsheet. Thus, we use BeautifulSoup to filter down this web page content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"create a BeautifulSoup instance using the content we extracted above\"\"\"\n",
    "soup = BeautifulSoup(r.content, 'html5lib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"extract all 'article' tags from HTML\"\"\"\n",
    "a = soup.find_all('article')\n",
    "articles = soup.find_all('h2',class_='entry-title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"convert article tags to strings\"\"\"\n",
    "str(articles[1].contents[1])\n",
    "\n",
    "articleStrings = []\n",
    "for i in range(len(articles)):\n",
    "    articleStrings.append(str(articles[i].contents[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extracting Dates with RegEx\n",
    "<a id=\"s2\"></a>\n",
    "\n",
    "Now that we've narrowed down the parts of the webpage we actually want to extract content from, we can use regular expressions to extract information like the date of publication from the HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "redates = []\n",
    "for i in range(len(articleStrings)):\n",
    "    redates.append(re.search(r'\\d{4}/\\d{2}/\\d{2}', articleStrings[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<_sre.SRE_Match object; span=(38, 48), match='2019/10/12'>,\n",
       " <_sre.SRE_Match object; span=(38, 48), match='2019/09/26'>,\n",
       " <_sre.SRE_Match object; span=(38, 48), match='2019/09/18'>,\n",
       " <_sre.SRE_Match object; span=(38, 48), match='2018/04/25'>,\n",
       " <_sre.SRE_Match object; span=(38, 48), match='2018/02/21'>,\n",
       " <_sre.SRE_Match object; span=(38, 48), match='2017/02/25'>,\n",
       " <_sre.SRE_Match object; span=(38, 48), match='2016/11/17'>,\n",
       " <_sre.SRE_Match object; span=(38, 48), match='2016/10/27'>,\n",
       " <_sre.SRE_Match object; span=(38, 48), match='2016/10/19'>,\n",
       " <_sre.SRE_Match object; span=(38, 48), match='2016/02/03'>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "redates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2019/10/12',\n",
       " '2019/09/26',\n",
       " '2019/09/18',\n",
       " '2018/04/25',\n",
       " '2018/02/21',\n",
       " '2017/02/25',\n",
       " '2016/11/17',\n",
       " '2016/10/27',\n",
       " '2016/10/19',\n",
       " '2016/02/03']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"CONVERT SRE_Match objects to strings\"\"\"\n",
    "dates = []\n",
    "for i in redates:\n",
    "    dates.append(i.group(0))\n",
    "dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Extracting Titles with String Indexing\n",
    "<a id=\"s3\"></a>\n",
    "\n",
    "If you look at a href tag for any given article in the search results page, you'll see that the title always appears after the first '>' and ends after the first '</'. We can use this fact to extract the titles of the articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = []\n",
    "for i in range(len(articleStrings)):\n",
    "    startSubStr = articleStrings[i].index('>')\n",
    "    endSubStr = articleStrings[i].index('</')\n",
    "    titles.append(articleStrings[i][startSubStr+1:endSubStr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tensions Rise Between Student Activists and Senior Administration Over Housing',\n",
       " 'Letter to the Editor: More than just displacement: the housing crisis and its debilitating effects',\n",
       " 'Dozens of students displaced due to on-campus housing issues',\n",
       " 'Affordable housing proposals cause for debate in the town of Wellesley',\n",
       " 'Office of Residential Life announces changes to the housing process',\n",
       " 'New housing process introduced at Wellesley',\n",
       " 'College revamps housing registration process',\n",
       " 'Off-campus housing diversifies studentsâ€™ residential and social experiences',\n",
       " 'Co-housing caters to the modern family',\n",
       " 'Wellesley students scramble to find housing upon returning from study abroad']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Writing Data to CSV\n",
    "<a id=\"s4\"></a>\n",
    "\n",
    "The output of this simple scraper is a CSV whose columns are the titles and dates of the articles on that given page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('wellesley_news_scraper_output.csv', 'w') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=[\"Title\", \"Date\"])\n",
    "    writer.writeheader()\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(zip(titles,dates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
